{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":281315401,"sourceType":"kernelVersion"},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport numpy as np\nimport os\n\nstart_time = time.time()\nfinal_cutoff_time = start_time + (4 * 60 + 58) * 60  # 4h 55m\n\nTOTAL_TIME = 4 * 60 * 60 + 58 * 60  # 4h 55m\nNUM_QUESTIONS = 50\nBUFFER_TIME = 60","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:29:31.697594Z","iopub.execute_input":"2026-01-05T15:29:31.697716Z","iopub.status.idle":"2026-01-05T15:29:31.700553Z","shell.execute_reply.started":"2026-01-05T15:29:31.697698Z","shell.execute_reply":"2026-01-05T15:29:31.700175Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import subprocess\n\nuninstall_proc = subprocess.Popen(\n    [\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:29:31.701295Z","iopub.execute_input":"2026-01-05T15:29:31.701418Z","iopub.status.idle":"2026-01-05T15:29:31.713419Z","shell.execute_reply.started":"2026-01-05T15:29:31.701407Z","shell.execute_reply":"2026-01-05T15:29:31.713029Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%time\n!find /kaggle/usr/lib -type f -print0 | xargs -0 -P 32 -n 500 cat > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:29:31.713822Z","iopub.execute_input":"2026-01-05T15:29:31.713968Z","iopub.status.idle":"2026-01-05T15:30:53.526334Z","shell.execute_reply.started":"2026-01-05T15:29:31.713956Z","shell.execute_reply":"2026-01-05T15:30:53.525710Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 513 ms, sys: 114 ms, total: 627 ms\nWall time: 1min 21s\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def cache_model(path, exts=(\".bin\", \".pt\", \".safetensors\"), num_workers=None, chunk_mb=256):\n    \"\"\"Pre-read model weight files into OS page cache.\"\"\"\n    import os\n    import multiprocessing\n    import time\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n\n    def warmup_file(fpath):\n        chunk_size = chunk_mb * 1024 * 1024\n        total = 0\n        with open(fpath, \"rb\") as f:\n            while True:\n                data = f.read(chunk_size)\n                if not data:\n                    break\n                total += len(data)\n        return fpath, total\n\n    if os.path.isdir(path):\n        files = [\n            os.path.join(root, name)\n            for root, _, names in os.walk(path)\n            for name in names\n            if name.endswith(exts)\n        ]\n        files.sort()\n    else:\n        files = [path]\n\n    if not files:\n        raise ValueError(f\"No model files found under: {path}\")\n\n    if num_workers is None:\n        try:\n            num_workers = min(multiprocessing.cpu_count(), 8)\n        except Exception:\n            num_workers = 4\n\n    print(f\"[cache_model] {len(files)} file(s), {num_workers} worker(s)\")\n    t0 = time.time()\n    total_bytes = 0\n\n    with ThreadPoolExecutor(max_workers=num_workers) as pool:\n        futures = {pool.submit(warmup_file, f): f for f in files}\n        for i, fut in enumerate(as_completed(futures), 1):\n            fpath, n = fut.result()\n            total_bytes += n\n            print(f\"[{i}/{len(files)}] cached {os.path.basename(fpath)}\")\n\n    elapsed = time.time() - t0\n    gb = total_bytes / 1024**3\n    print(f\"[cache_model] total read ‚âà {gb:.2f} GB in {elapsed:.2f}s\")\n    return total_bytes\n\n\ncache_model(\"/kaggle/input/gpt-oss-120b/transformers/default/1\", num_workers=16, chunk_mb=1024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:30:53.526964Z","iopub.execute_input":"2026-01-05T15:30:53.527095Z","iopub.status.idle":"2026-01-05T15:32:04.804675Z","shell.execute_reply.started":"2026-01-05T15:30:53.527079Z","shell.execute_reply":"2026-01-05T15:32:04.804288Z"}},"outputs":[{"name":"stdout","text":"[cache_model] 15 file(s), 16 worker(s)\n[1/15] cached model-00007-of-00014.safetensors\n[2/15] cached model-00011-of-00014.safetensors\n[3/15] cached model-00001-of-00014.safetensors\n[4/15] cached model-00005-of-00014.safetensors\n[5/15] cached model-00003-of-00014.safetensors\n[6/15] cached model-00014-of-00014.safetensors\n[7/15] cached model-00009-of-00014.safetensors\n[8/15] cached model-00012-of-00014.safetensors\n[9/15] cached model-00008-of-00014.safetensors\n[10/15] cached model-00013-of-00014.safetensors\n[11/15] cached model-00004-of-00014.safetensors\n[12/15] cached model-00000-of-00014.safetensors\n[13/15] cached model-00002-of-00014.safetensors\n[14/15] cached model-00010-of-00014.safetensors\n[15/15] cached model-00006-of-00014.safetensors\n[cache_model] total read ‚âà 60.77 GB in 71.27s\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"65248893184"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"%%time\n# Copy vLLM compile cache if available\nimport os\nif os.path.exists(\"/kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache\"):\n    !mkdir -p /root/.cache/vllm/\n    !cp -r /kaggle/input/gpt-oss-120b-cache-compile/torch_compile_cache /root/.cache/vllm/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.805612Z","iopub.execute_input":"2026-01-05T15:32:04.805740Z","iopub.status.idle":"2026-01-05T15:32:04.808984Z","shell.execute_reply.started":"2026-01-05T15:32:04.805730Z","shell.execute_reply":"2026-01-05T15:32:04.808582Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 0 ns, sys: 286 ¬µs, total: 286 ¬µs\nWall time: 249 ¬µs\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"uninstall_proc.wait()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.809403Z","iopub.execute_input":"2026-01-05T15:32:04.809514Z","iopub.status.idle":"2026-01-05T15:32:04.828289Z","shell.execute_reply.started":"2026-01-05T15:32:04.809505Z","shell.execute_reply":"2026-01-05T15:32:04.827851Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.828729Z","iopub.execute_input":"2026-01-05T15:32:04.828842Z","iopub.status.idle":"2026-01-05T15:32:04.849500Z","shell.execute_reply.started":"2026-01-05T15:32:04.828832Z","shell.execute_reply":"2026-01-05T15:32:04.849124Z"}},"outputs":[{"name":"stdout","text":"cl100k_base.tiktoken\no200k_base.tiktoken\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['ls', '/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings'], returncode=0)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.849995Z","iopub.execute_input":"2026-01-05T15:32:04.850114Z","iopub.status.idle":"2026-01-05T15:32:04.852871Z","shell.execute_reply.started":"2026-01-05T15:32:04.850104Z","shell.execute_reply":"2026-01-05T15:32:04.852475Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Python Tool with Jupyter Kernel","metadata":{}},{"cell_type":"code","source":"%%writefile local_python_tool.py\n\"\"\"Python tool using Jupyter kernel for stateful execution.\"\"\"\nimport os\nimport queue\nimport threading\nfrom abc import ABC, abstractmethod\nfrom typing import AsyncIterator, Any\nfrom uuid import UUID, uuid4\n\nfrom openai_harmony import (\n    Author,\n    Content,\n    Message,\n    Role,\n    TextContent,\n    ToolNamespaceConfig,\n)\n\n\ndef add_libs(code: str) -> str:\n    \"\"\"Add common math libraries to code.\"\"\"\n    return \"import math\\nimport numpy as np\\nimport sympy as sp\\nfrom sympy import *\\n\" + code\n\n\ndef ensure_last_print(code: str) -> str:\n    \"\"\"Ensure the last expression is printed.\"\"\"\n    lines = code.strip().split(\"\\n\")\n    if lines and \"print(\" not in lines[-1] and \"import\" not in lines[-1]:\n        if \"#\" in lines[-1]:\n            lines[-1] = lines[-1].split(\"#\")[0]\n        lines[-1] = \"print(\" + lines[-1] + \")\"\n    return \"\\n\".join(lines)\n\n\nclass LocalJupyterSession:\n    \"\"\"Stateful Jupyter kernel session for code execution.\"\"\"\n\n    # Class-level lock and port counter to avoid port conflicts\n    _port_lock = threading.Lock()\n    _next_port = 50000\n    _max_port = 65535  # Maximum valid port number\n\n    @classmethod\n    def _get_next_ports(cls, count: int = 5) -> list[int]:\n        \"\"\"Get next available ports for kernel connection.\"\"\"\n        import socket\n        with cls._port_lock:\n            ports = []\n            attempts = 0\n            max_attempts = 100  # Prevent infinite loop\n            \n            while len(ports) < count and attempts < max_attempts:\n                start_port = cls._next_port\n                # Check if port range is available\n                available = True\n                for i in range(count):\n                    port = start_port + i\n                    if port > cls._max_port:\n                        # Wrap around to beginning of port range\n                        start_port = 50000\n                        port = start_port + i\n                    \n                    # Quick check if port is in use\n                    try:\n                        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                            s.settimeout(0.1)\n                            result = s.connect_ex(('127.0.0.1', port))\n                            if result == 0:\n                                available = False\n                                break\n                    except Exception:\n                        # If check fails, assume port might be in use\n                        available = False\n                        break\n                \n                if available:\n                    ports = list(range(start_port, start_port + count))\n                    cls._next_port = start_port + count\n                    if cls._next_port > cls._max_port:\n                        cls._next_port = 50000\n                    break\n                else:\n                    # Try next range\n                    cls._next_port += count\n                    if cls._next_port > cls._max_port:\n                        cls._next_port = 50000\n                    attempts += 1\n            \n            if len(ports) < count:\n                # Fallback: just return sequential ports without checking\n                ports = list(range(cls._next_port, cls._next_port + count))\n                cls._next_port += count\n                if cls._next_port > cls._max_port:\n                    cls._next_port = 50000\n            \n            return ports\n\n    def __init__(self, connection_file: str | None = None, *, timeout: float = 120.0):\n        try:\n            from jupyter_client import BlockingKernelClient, KernelManager\n        except ImportError as exc:\n            raise RuntimeError(\"jupyter_client package required\") from exc\n\n        self._default_timeout = timeout\n        self._owns_kernel = False\n        self._client: BlockingKernelClient\n        self._km: KernelManager | None = None\n\n        if connection_file:\n            from pathlib import Path\n            connection_path = Path(connection_file).expanduser()\n            if not connection_path.exists():\n                raise FileNotFoundError(f\"Connection file not found: {connection_path}\")\n            client = BlockingKernelClient()\n            client.load_connection_file(str(connection_path))\n            client.start_channels()\n            client.wait_for_ready(timeout=self._default_timeout)\n            self._client = client\n        else:\n            # Allocate unique ports to avoid conflicts when running multiple kernels\n            ports = self._get_next_ports(5)\n            km = None\n            max_retries = 3\n            for retry in range(max_retries):\n                try:\n                    km = KernelManager()\n                    km.shell_port = ports[0]\n                    km.iopub_port = ports[1]\n                    km.stdin_port = ports[2]\n                    km.hb_port = ports[3]\n                    km.control_port = ports[4]\n                    km.start_kernel()\n                    client = km.blocking_client()\n                    client.start_channels()\n                    client.wait_for_ready(timeout=self._default_timeout)\n                    self._client = client\n                    self._km = km\n                    self._owns_kernel = True\n                    break\n                except Exception as e:\n                    if retry < max_retries - 1:\n                        # Try different ports\n                        ports = self._get_next_ports(5)\n                        if km is not None:\n                            try:\n                                km.shutdown_kernel(now=True)\n                            except Exception:\n                                pass\n                    else:\n                        # Last retry failed, raise the exception\n                        raise RuntimeError(f\"Failed to start kernel after {max_retries} retries: {e}\") from e\n\n    def execute(self, code: str, *, timeout: float | None = None) -> str:\n        \"\"\"Execute code and return combined stdout/stderr.\n        timeout: WALL-CLOCK seconds limit for this execution.\n        \"\"\"\n        import time\n        import queue as _queue\n    \n        client = self._client\n        effective_timeout = float(timeout or self._default_timeout)\n    \n        msg_id = client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n    \n        stdout_parts: list[str] = []\n        stderr_parts: list[str] = []\n        \n        # Track if we've seen a timeout/interrupt to filter IPython internal errors\n        _timeout_triggered = False\n    \n        start = time.time()\n        poll = 0.5  # seconds: small polling interval so we can enforce wall-clock timeout\n    \n        def _timed_out() -> bool:\n            return (time.time() - start) >= effective_timeout\n    \n        # iopub loop\n        max_timeout_grace = 1.0  # Give kernel 1 seconds to clean up after interrupt\n        timeout_grace_start = None\n        \n        while True:\n            if _timed_out():\n                if not _timeout_triggered:\n                    _timeout_triggered = True\n                    timeout_grace_start = time.time()\n                    # interrupt the kernel to stop runaway execution\n                    try:\n                        # BlockingKernelClient usually has interrupt_kernel\n                        client.interrupt_kernel()\n                    except Exception:\n                        try:\n                            if self._owns_kernel and self._km is not None:\n                                self._km.interrupt_kernel()\n                        except Exception:\n                            pass\n                \n                # After grace period, stop collecting messages and raise timeout\n                if timeout_grace_start and (time.time() - timeout_grace_start) > max_timeout_grace:\n                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n    \n            try:\n                msg = client.get_iopub_msg(timeout=poll)\n            except _queue.Empty:\n                if _timeout_triggered and timeout_grace_start and (time.time() - timeout_grace_start) > max_timeout_grace:\n                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n                continue\n    \n            if msg.get(\"parent_header\", {}).get(\"msg_id\") != msg_id:\n                continue\n    \n            msg_type = msg.get(\"msg_type\")\n            content = msg.get(\"content\", {})\n            \n            # After timeout is triggered, only collect essential messages and filter IPython errors\n            if _timeout_triggered:\n                # Only process status messages to detect idle state, ignore everything else\n                if msg_type == \"status\":\n                    if content.get(\"execution_state\") == \"idle\":\n                        break\n                # Skip all other messages after timeout to avoid IPython internal errors\n                continue\n    \n            if msg_type == \"stream\":\n                text = content.get(\"text\", \"\")\n                if content.get(\"name\") == \"stdout\":\n                    stdout_parts.append(text)\n                else:\n                    stderr_parts.append(text)\n            elif msg_type == \"error\":\n                traceback_data = content.get(\"traceback\")\n                if traceback_data:\n                    stderr_parts.append(\"\\n\".join(traceback_data))\n                else:\n                    ename = content.get(\"ename\", \"\")\n                    evalue = content.get(\"evalue\", \"\")\n                    stderr_parts.append(f\"{ename}: {evalue}\".strip())\n            elif msg_type in {\"execute_result\", \"display_data\"}:\n                data = content.get(\"data\", {})\n                text = data.get(\"text/plain\")\n                if text:\n                    stdout_parts.append(text if text.endswith(\"\\n\") else f\"{text}\\n\")\n            elif msg_type == \"status\" and content.get(\"execution_state\") == \"idle\":\n                break\n    \n        # shell reply (also wall-time protected)\n        # Reuse timeout_grace_start from iopub loop if timeout was already triggered\n        shell_timeout_grace_start = timeout_grace_start if _timeout_triggered else None\n        \n        while True:\n            if _timed_out():\n                if not _timeout_triggered:\n                    _timeout_triggered = True\n                    shell_timeout_grace_start = time.time()\n                    try:\n                        client.interrupt_kernel()\n                    except Exception:\n                        try:\n                            if self._owns_kernel and self._km is not None:\n                                self._km.interrupt_kernel()\n                        except Exception:\n                            pass\n                \n                # After grace period, stop collecting messages and raise timeout\n                if shell_timeout_grace_start and (time.time() - shell_timeout_grace_start) > max_timeout_grace:\n                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n    \n            try:\n                reply = client.get_shell_msg(timeout=poll)\n            except _queue.Empty:\n                if _timeout_triggered and shell_timeout_grace_start and (time.time() - shell_timeout_grace_start) > max_timeout_grace:\n                    raise TimeoutError(f\"Python execution exceeded wall-time limit: {effective_timeout:.1f}s\")\n                continue\n    \n            if reply.get(\"parent_header\", {}).get(\"msg_id\") != msg_id:\n                continue\n    \n            reply_content = reply.get(\"content\", {})\n            \n            # After timeout, skip error messages to avoid IPython internal errors\n            if _timeout_triggered and reply_content.get(\"status\") == \"error\":\n                # Skip IPython internal errors, just break to exit\n                break\n            \n            if reply_content.get(\"status\") == \"error\":\n                traceback_data = reply_content.get(\"traceback\")\n                if traceback_data:\n                    stderr_parts.append(\"\\n\".join(traceback_data))\n                else:\n                    ename = reply_content.get(\"ename\", \"\")\n                    evalue = reply_content.get(\"evalue\", \"\")\n                    stderr_parts.append(f\"{ename}: {evalue}\".strip())\n            break\n    \n        stdout = \"\".join(stdout_parts)\n        stderr = \"\".join(stderr_parts)\n    \n        if stderr:\n            stdout = f\"{stdout.rstrip()}\\n{stderr}\" if stdout else stderr\n        if not stdout.strip():\n            stdout = \"[WARN] No output. Use print() to see results.\"\n        return stdout\n\n\n    def close(self):\n        import contextlib\n        with contextlib.suppress(Exception):\n            self._client.stop_channels()\n        if self._owns_kernel and self._km is not None:\n            with contextlib.suppress(Exception):\n                self._km.shutdown_kernel(now=True)\n\n    def __del__(self):\n        self.close()\n\n\nclass PythonTool:\n    \"\"\"Python execution tool using Jupyter kernel.\"\"\"\n\n    def __init__(self, execution_backend: str | None = None, local_jupyter_timeout: float = 60.0):\n        self._local_jupyter_timeout = local_jupyter_timeout\n        self._execution_lock = threading.Lock()\n        self._jupyter_session: LocalJupyterSession | None = None\n        # Lazy initialization to avoid port conflicts during object creation\n        self._init_lock = threading.Lock()\n\n    def _ensure_session(self):\n        \"\"\"Lazily initialize the Jupyter session.\"\"\"\n        if self._jupyter_session is None:\n            with self._init_lock:\n                if self._jupyter_session is None:\n                    self._jupyter_session = LocalJupyterSession(timeout=self._local_jupyter_timeout)\n\n    @classmethod\n    def get_tool_name(cls) -> str:\n        return \"python\"\n\n    @property\n    def name(self) -> str:\n        return self.get_tool_name()\n\n    @property\n    def instruction(self) -> str:\n        return \"\"\"Use this tool to execute Python code. The code runs in a stateful Jupyter notebook. Use print() to see output.\"\"\"\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        return ToolNamespaceConfig(\n            name=self.get_tool_name(),\n            description=self.instruction,\n            tools=[]\n        )\n\n    def _make_response(self, output: str, channel: str | None = None) -> Message:\n        content = TextContent(text=output)\n        author = Author(role=Role.TOOL, name=self.get_tool_name())\n        message = Message(author=author, content=[content]).with_recipient(\"assistant\")\n        if channel:\n            message = message.with_channel(channel)\n        return message\n\n    def process_sync_plus(self, message: Message, timeout: float | None = None) -> list[Message]:\n        \"\"\"Execute code from message using Jupyter kernel.\"\"\"\n        self._ensure_session()\n        script = message.content[0].text\n        with self._execution_lock:\n            try:\n                output = self._jupyter_session.execute(script, timeout=timeout)\n            except TimeoutError as exc:\n                output = f\"[ERROR] {exc}\"\n            except Exception as exc:\n                output = f\"[ERROR] {exc}\"\n        return [self._make_response(output, channel=message.channel)]\n\n    def close(self):\n        if self._jupyter_session is not None:\n            self._jupyter_session.close()\n            self._jupyter_session = None\n\n    def __del__(self):\n        self.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.853444Z","iopub.execute_input":"2026-01-05T15:32:04.853564Z","iopub.status.idle":"2026-01-05T15:32:04.867620Z","shell.execute_reply.started":"2026-01-05T15:32:04.853555Z","shell.execute_reply":"2026-01-05T15:32:04.867221Z"}},"outputs":[{"name":"stdout","text":"Writing local_python_tool.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Imports and Setup","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport re\nimport math\nimport threading\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List\n\nimport pandas as pd\nimport polars as pl\nfrom openai import OpenAI\nfrom transformers import set_seed, AutoTokenizer\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport threading\n\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n    Conversation,\n    Message,\n    Role,\n    SystemContent,\n    ReasoningEffort,\n    RenderConversationConfig,\n)\n\nfrom local_python_tool import PythonTool\n\n# Load Harmony encoding for GPT-OSS\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n# Constants\nSEED = 42\nset_seed(SEED)\nMAX_LEN = 64 * 1024\nUSE_BUDGET = False\nK = 8\n\n# Inference parameters \nTEMPERATURE = 1\nTOP_P = 1\nMIN_P = 0.015","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:04.868057Z","iopub.execute_input":"2026-01-05T15:32:04.868162Z","iopub.status.idle":"2026-01-05T15:32:38.771968Z","shell.execute_reply.started":"2026-01-05T15:32:04.868153Z","shell.execute_reply":"2026-01-05T15:32:38.771528Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class DynamicTimeBudget:\n    \"\"\"Manages dynamic time allocation with rollover from early stopping.\"\"\"\n    \n    def __init__(self, total_time_seconds: float, num_questions: int, buffer_seconds: float = 60):\n        self.total_time = total_time_seconds\n        self.num_questions = num_questions\n        self.buffer = buffer_seconds\n        self.start_time = time.time()\n        \n        # Available time excluding buffer\n        self.available_time = total_time_seconds - buffer_seconds\n        \n        # Track time usage\n        self.time_used = 0\n        self.questions_completed = 0\n        self.time_saved = 0  # Accumulated time from early stops\n        \n    def get_deadline_for_question(self) -> float:\n        \"\"\"Calculate deadline for current question with rollover time.\"\"\"\n        questions_remaining = self.num_questions - self.questions_completed\n        \n        if questions_remaining <= 0:\n            return time.time() + 60  # Emergency fallback\n        \n        # Base time per remaining question\n        time_remaining = self.available_time - self.time_used\n        base_time = time_remaining / questions_remaining\n        \n        # Add any saved time from early stopping\n        allocated_time = base_time + self.time_saved\n        \n        # Reset saved time (it's now allocated to this question)\n        self.time_saved = 0\n        \n        deadline = time.time() + allocated_time\n        \n        print(f\"‚è±Ô∏è  Allocated {allocated_time:.1f}s for question {self.questions_completed + 1}\")\n        print(f\"   (Base: {base_time:.1f}s, Rollover: {self.time_saved:.1f}s, Remaining: {questions_remaining} questions)\")\n        \n        return deadline\n    \n    def record_question_completion(self, time_spent: float, early_stopped: bool = False):\n        \"\"\"Record completion and calculate time savings.\"\"\"\n        self.time_used += time_spent\n        self.questions_completed += 1\n        \n        # If early stopped, calculate how much time was saved\n        if early_stopped:\n            questions_remaining = self.num_questions - self.questions_completed\n            if questions_remaining > 0:\n                expected_time = (self.available_time - self.time_used + time_spent) / (questions_remaining + 1)\n                time_saved = max(0, expected_time - time_spent)\n                self.time_saved += time_saved\n                print(f\"üí∞ Early stop saved {time_saved:.1f}s (total saved: {self.time_saved:.1f}s)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.772492Z","iopub.execute_input":"2026-01-05T15:32:38.772735Z","iopub.status.idle":"2026-01-05T15:32:38.778081Z","shell.execute_reply.started":"2026-01-05T15:32:38.772723Z","shell.execute_reply":"2026-01-05T15:32:38.777676Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Start vLLM Server","metadata":{}},{"cell_type":"code","source":"def start_vllm_server() -> subprocess.Popen:\n    \"\"\"Start vLLM server in background.\"\"\"\n    command = [\n        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n        \"--model\", \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n        \"--served-model-name\", \"gpt-oss\",\n        \"--tensor-parallel-size\", \"1\",\n        \"--max-num-seqs\", \"64\",\n        \"--gpu-memory-utilization\", \"0.96\",\n        \"--host\", \"0.0.0.0\",\n        \"--port\", \"8000\",\n        \"--dtype\", \"auto\",\n        \"--max-model-len\", str(MAX_LEN),\n        \"--stream-interval\", \"20\",\n    ]\n    with open(\"./vllm.log\", \"w\") as logfile:\n        process = subprocess.Popen(\n            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n        )\n    print(\"vLLM server started. Logs: ./vllm.log\")\n    return process\n\n\nvllm_process = start_vllm_server()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.778516Z","iopub.execute_input":"2026-01-05T15:32:38.778637Z","iopub.status.idle":"2026-01-05T15:32:38.809450Z","shell.execute_reply.started":"2026-01-05T15:32:38.778628Z","shell.execute_reply":"2026-01-05T15:32:38.809037Z"}},"outputs":[{"name":"stdout","text":"vLLM server started. Logs: ./vllm.log\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# TIR Prompts","metadata":{}},{"cell_type":"code","source":"# Option A: Exact same as way-to-30 (proven 30/50 on LB)\nTIR_PROMPT_SIMPLE0 = \"\"\"You are an elite olympiad mathematician solving a national or international level problem.Work with full mathematical rigor: clearly justify every nontrivial step, consider alternative solution paths when they provide insight or validation, and carefully analyze edge cases and hidden assumptions.If computation or verification is helpful, use Python to confirm correctness.Do not rely on intuition alone and do not guess.Conclude only after verifying the result, and output solely the final answer in the form \\boxed{n}, where n ‚àà [0,99999].\"\"\"\n\nTIR_PROMPTS = [TIR_PROMPT_SIMPLE0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.809872Z","iopub.execute_input":"2026-01-05T15:32:38.810003Z","iopub.status.idle":"2026-01-05T15:32:38.823870Z","shell.execute_reply.started":"2026-01-05T15:32:38.809992Z","shell.execute_reply":"2026-01-05T15:32:38.823525Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Inferencer with Harmony Protocol","metadata":{}},{"cell_type":"code","source":"import queue\nfrom local_python_tool import PythonTool\n\npython_pool = queue.Queue(maxsize=K)\n\nfor _ in range(K):\n    t = PythonTool(execution_backend=\"jupyter\", local_jupyter_timeout=60.0)\n    python_pool.put(t)\nprint(\"Pool created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.825171Z","iopub.execute_input":"2026-01-05T15:32:38.825292Z","iopub.status.idle":"2026-01-05T15:32:38.834461Z","shell.execute_reply.started":"2026-01-05T15:32:38.825283Z","shell.execute_reply":"2026-01-05T15:32:38.834059Z"}},"outputs":[{"name":"stdout","text":"Pool created!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import gc\n\nCLEANUP_CODE = r\"\"\"\nimport gc\n_keep = {\n    \"__builtins__\", \"__name__\", \"__doc__\", \"__package__\", \"__loader__\", \"__spec__\",\n    \"np\", \"sp\", \"math\",\n}\ng = globals()\nfor k in list(g.keys()):\n    if k in _keep or k.startswith(\"_\"):\n        continue\n    try:\n        del g[k]\n    except Exception:\n        pass\ngc.collect()\n\"\"\"\nprint(\"yes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.834899Z","iopub.execute_input":"2026-01-05T15:32:38.835049Z","iopub.status.idle":"2026-01-05T15:32:38.846019Z","shell.execute_reply.started":"2026-01-05T15:32:38.835037Z","shell.execute_reply":"2026-01-05T15:32:38.845639Z"}},"outputs":[{"name":"stdout","text":"yes\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"class HarmonyTIRInferencer:\n    \"\"\"Inferencer using Harmony protocol with Tool-Integrated Reasoning (TIR).\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        max_model_len: int = MAX_LEN,\n        temperature: float = TEMPERATURE,\n        top_p: float = TOP_P,\n        min_p: float = MIN_P,\n        seed: int = SEED,\n        k: int = K,\n        use_budget: bool = USE_BUDGET,\n        max_iter: int = 100,\n    ):\n        self.model_path = model_path\n        self.model = \"gpt-oss\"\n        self.max_model_len = max_model_len\n        self.temperature = temperature\n        self.top_p = top_p\n        self.min_p = min_p\n        self.seed = seed\n        self.k = k\n        self.use_budget = use_budget\n        self.max_iter = max_iter\n        self.base_budget = 60 * 5.5  # 5.5 minutes base per problem\n        self.budget = 370              # initial budget in seconds (~6.1 min for first problem)\n        self.deadline = None\n\n        # Initialize the OpenAI-compatible client pointing to local vLLM server\n        self.client = OpenAI(\n            base_url=\"http://127.0.0.1:8000/v1\",\n            api_key=\"sk-local\",\n            timeout=360,\n        )\n        self.stop_token_ids = encoding.stop_tokens_for_assistant_actions()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n    def wait_server(self):\n        \"\"\"Wait until the vLLM server is ready to accept requests.\"\"\"\n        for _ in range(15 * 60):\n            time.sleep(1)\n            try:\n                # List models to check if server is up\n                print(self.client.models.list())\n                return\n            except Exception:\n                continue\n        raise RuntimeError(\"vLLM server failed to start\")\n\n    def get_num_samples(self) -> int:\n        \"\"\"Determine number of parallel samples to generate based on remaining budget.\"\"\"\n        if not self.use_budget:\n            print(f\"Budget disabled -> N: {self.k}\")\n            return self.k\n        else:\n            return self.k\n            \n    def apply_chat_template(self, prompt: str, python_tool: PythonTool) -> list[Message]:\n        \"\"\"Wrap user prompt into Harmony conversation format with system and tool info.\"\"\"\n        return [\n            Message.from_role_and_content(\n                Role.SYSTEM,\n                SystemContent.new()\n                .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n                .with_tools(python_tool.tool_config)\n            ),\n            Message.from_role_and_content(Role.USER, prompt),\n        ]\n\n    def format_prompts(self, problem: str) -> list[str]:\n        \"\"\"Create multiple prompts (possibly with different TIR strategies) for one problem.\"\"\"\n        num_samples = self.get_num_samples()\n        prompts = []\n        for i in range(num_samples):\n            # Alternate between the prompt templates for diversity\n            tir_prompt = TIR_PROMPTS[i % len(TIR_PROMPTS)]\n            prompts.append(problem + \"\\n\\n\" + tir_prompt)\n        return prompts\n\n    def inference(self, problem: str, deadline: float) -> tuple[int, float]:\n        \"\"\"Run the multi-sample inference for a single problem and return the final answer and saved time.\"\"\"\n        self.deadline = deadline\n        start_time = time.time()\n    \n        prompts = self.format_prompts(problem)\n        responses = self._inference_parallel(prompts)\n    \n        duration = time.time() - start_time\n        saved_time = max(0.0, deadline - time.time())\n    \n        print(f\"[Budget]: {(deadline - start_time):.2f}s\")\n        print(f\"[inference] Took {duration:.2f}s\")\n        print(f\"[Saved time]: {saved_time:.2f}s\")\n    \n        return self.parse_responses(responses), saved_time\n\n    \n    def single_generate_tir(self, prompt: str, stop_event: threading.Event, seed_offset: int = 0) -> str:\n        \"\"\"Generate single TIR response with tool execution (dynamic timeouts).\"\"\"\n        python_tool = None\n    \n        def _compute_req_timeout() -> float:\n            # For vLLM request timeout\n            CUSHION = 0.5\n            MAX_REQ_TIMEOUT = 30.0\n            MIN_ALLOW = 0.2\n    \n            if not getattr(self, \"deadline\", None):\n                return MAX_REQ_TIMEOUT\n    \n            remaining = self.deadline - time.time()\n            if remaining <= 0:\n                return 0.0\n    \n            t = remaining - CUSHION\n            if t <= 0:\n                return 0.0\n    \n            return min(MAX_REQ_TIMEOUT, max(MIN_ALLOW, t))\n    \n        def _compute_py_timeout() -> float:\n            # For python tool timeout\n            PY_CUSHION = 0.3\n            MAX_PY_TIMEOUT = 30.0\n            MIN_ALLOW = 2.0\n    \n            if not getattr(self, \"deadline\", None):\n                return MAX_PY_TIMEOUT\n    \n            remaining = self.deadline - time.time()\n            t = remaining - PY_CUSHION\n            if t <= 0:\n                return 0.0\n    \n            return min(MAX_PY_TIMEOUT, max(MIN_ALLOW, t))\n    \n        try:\n            # Use pool instead of creating new PythonTool\n            try:\n                python_tool = python_pool.get(timeout=30.0)\n            except queue.Empty:\n                print(\"‚ö†Ô∏è Failed to get python_tool from pool, creating new one\")\n                python_tool = PythonTool(execution_backend=\"jupyter\")\n                try:\n                    python_tool._ensure_session()\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è python session init failed: {e}\")\n                    if python_tool is not None:\n                        try:\n                            python_tool.close()\n                        except Exception:\n                            pass\n                    return \"\"\n            else:\n                # Verify session is still alive\n                try:\n                    if python_tool._jupyter_session is None:\n                        python_tool._ensure_session()\n                    # Quick health check: try to execute a simple command\n                    test_output = python_tool._jupyter_session.execute(\"1+1\", timeout=2.0)\n                    if \"[ERROR]\" in test_output or \"Traceback\" in test_output:\n                        # Session is broken, recreate it\n                        try:\n                            python_tool.close()\n                        except Exception:\n                            pass\n                        python_tool._jupyter_session = None\n                        python_tool._ensure_session()\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è python session health check failed: {e}, recreating\")\n                    try:\n                        python_tool.close()\n                    except Exception:\n                        pass\n                    python_tool._jupyter_session = None\n                    try:\n                        python_tool._ensure_session()\n                    except Exception as e2:\n                        print(f\"‚ö†Ô∏è python session recreate failed: {e2}\")\n                        try:\n                            python_pool.put(python_tool, block=False)\n                        except queue.Full:\n                            pass\n                        return \"\"\n    \n            messages = self.apply_chat_template(prompt, python_tool)\n            final_answer_found = \"\"\n    \n            for iteration in range(self.max_iter):\n                # termination checks\n                if stop_event and stop_event.is_set():\n                    print(\"üõë Stop signal received\")\n                    break\n                if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n                    print(\"‚è∞ Deadline reached\")\n                    break\n                if final_answer_found:\n                    break\n    \n                prompt_ids = encoding.render_conversation_for_completion(\n                    Conversation.from_messages(messages), Role.ASSISTANT\n                )\n                max_tokens = self.max_model_len - len(prompt_ids)\n                if max_tokens < 1:\n                    print(\"‚ö†Ô∏è Context full\")\n                    break\n    \n                req_timeout = _compute_req_timeout()\n                if req_timeout <= 0:\n                    print(\"‚è∞ Not enough remaining time for vLLM request\")\n                    break\n    \n                token_buffer: list[int] = []\n                token_buffer_str = \"\"\n                breaking = False\n    \n                stream = None\n                try:\n                    stream = self.client.completions.create(\n                        model=self.model,\n                        prompt=prompt_ids,\n                        max_tokens=max_tokens,\n                        temperature=self.temperature,\n                        top_p=self.top_p,\n                        seed=self.seed + seed_offset,\n                        stream=True,\n                        extra_body=dict(\n                            min_p=self.min_p,\n                            stop_token_ids=self.stop_token_ids,\n                            return_token_ids=True,\n                        ),\n                        timeout=req_timeout,\n                    )\n    \n                    for chunk in stream:\n                        try:\n                            if stop_event and stop_event.is_set():\n                                breaking = True\n                                break\n                            if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n                                breaking = True\n                                break\n                            \n                            # Safely extract chunk data\n                            if not chunk.choices or len(chunk.choices) == 0:\n                                continue\n                            \n                            choice = chunk.choices[0]\n                            token_chunk = getattr(choice, 'token_ids', None) or []\n                            text_chunk = getattr(choice, 'text', '') or ''\n    \n                            if token_chunk:\n                                token_buffer.extend(token_chunk)\n                                token_buffer_str += text_chunk\n    \n                            if len(token_buffer) > 60_000:\n                                print(\"‚ö†Ô∏è Token limit\")\n                                breaking = True\n                                break\n    \n                            # early stop when boxed appears\n                            if \"}\" in text_chunk and self.extract_boxed_text(token_buffer_str) is not None:\n                                final_answer_found = token_buffer_str\n                                breaking = True\n                                break\n                        except StopIteration:\n                            # Stream ended normally\n                            break\n                        except Exception as e:\n                            print(f\"‚ö†Ô∏è Error processing stream chunk: {e}\")\n                            # Continue processing, but mark as potentially broken\n                            break\n    \n                except Exception as e:\n                    print(f\"‚ö†Ô∏è Error creating/reading stream: {e}\")\n                    breaking = True\n                finally:\n                    if stream is not None:\n                        try:\n                            stream.close()\n                        except Exception:\n                            pass\n                        # Additional cleanup attempt\n                        try:\n                            del stream\n                        except Exception:\n                            pass\n    \n                if breaking:\n                    break\n    \n                if not token_buffer:\n                    continue\n    \n                # parse completion\n                try:\n                    new_messages = encoding.parse_messages_from_completion_tokens(\n                        token_buffer, Role.ASSISTANT\n                    )\n                except Exception as e:\n                    print(f\"Error parsing completion: {e}\")\n                    break\n    \n                messages.extend(new_messages)\n                last_message = messages[-1]\n    \n                if last_message.channel == \"final\" or token_buffer[-1] == 200002:\n                    break\n    \n                if last_message.recipient == \"python\":\n                    if stop_event and stop_event.is_set():\n                        break\n                    if getattr(self, \"deadline\", None) and time.time() >= self.deadline:\n                        break\n    \n                    py_timeout = _compute_py_timeout()\n                    if py_timeout <= 0 or py_timeout < 0.5:\n                        print(f\"‚è∞ Not enough remaining time for python ({py_timeout:.2f}s)\")\n                        break\n    \n                    try:\n                        response_msgs = python_tool.process_sync_plus(last_message, timeout=py_timeout)\n                    except Exception as e:\n                        # treat any python tool failure as terminal for this sample\n                        print(f\"‚ö†Ô∏è python tool failed: {e}\")\n                        break\n    \n                    messages.extend(response_msgs)\n    \n            if final_answer_found:\n                return final_answer_found\n    \n            return encoding.decode_utf8(\n                encoding.render_conversation_for_training(\n                    Conversation.from_messages(messages),\n                    RenderConversationConfig(auto_drop_analysis=False),\n                )\n            )\n    \n        except KeyboardInterrupt:\n            # never swallow manual interrupts\n            raise\n        except Exception as e:\n            import traceback\n            print(f\"Error in generation: {e}\")\n            print(f\"Traceback: {traceback.format_exc()}\")\n            return \"\"\n        finally:\n            # Return tool to pool instead of closing it\n            if python_tool is not None:\n                try:\n                    # Only return to pool if we got it from pool\n                    # Check if tool is still healthy before returning\n                    if python_tool._jupyter_session is not None:\n                        try:\n                            # Quick health check\n                            test_output = python_tool._jupyter_session.execute(\"1+1\", timeout=1.0)\n                            if \"[ERROR]\" not in test_output and \"Traceback\" not in test_output:\n                                # Tool is healthy, return to pool\n                                try:\n                                    python_pool.put(python_tool, block=False)\n                                except queue.Full:\n                                    # Pool is full, close the tool\n                                    python_tool.close()\n                            else:\n                                # Tool is broken, close it\n                                python_tool.close()\n                        except Exception:\n                            # Health check failed, close the tool\n                            try:\n                                python_tool.close()\n                            except Exception:\n                                pass\n                    else:\n                        # No session, safe to return to pool\n                        try:\n                            python_pool.put(python_tool, block=False)\n                        except queue.Full:\n                            pass\n                except Exception as e:\n                    # If anything goes wrong, try to close the tool\n                    try:\n                        python_tool.close()\n                    except Exception:\n                        pass\n            \n\n    def _inference_parallel(self, prompts: list[str]) -> list[str]:\n        \"\"\"Run multiple `single_generate_tir` in parallel and return all raw responses.\"\"\"\n        stop_event = threading.Event()\n        answers_collected: List[int] = []\n        raw_responses = [\"\"] * len(prompts)\n        majority_threshold = len(prompts) / 2  # more than half of the samples\n    \n        print(f\"üöÄ Sampling {len(prompts)} times (threshold: > {majority_threshold})...\")\n    \n        executor = ThreadPoolExecutor(max_workers=self.k)\n        futures = []\n        future_to_idx = {}\n        try:\n            for i, p in enumerate(prompts):\n                fut = executor.submit(self.single_generate_tir, p, stop_event, i)\n                futures.append(fut)\n                future_to_idx[fut] = i\n    \n            completed_count = 0\n            for fut in as_completed(futures):\n                idx = future_to_idx.get(fut, -1)\n                if idx < 0:\n                    continue\n                    \n                try:\n                    result_text = fut.result(timeout=1.0)\n                except Exception as e:\n                    import traceback\n                    print(f\"Task exception for idx {idx}: {e}\")\n                    print(f\"Traceback: {traceback.format_exc()}\")\n                    result_text = \"\"\n                \n                raw_responses[idx] = result_text\n                completed_count += 1\n    \n                ans = self.extract_boxed_text(result_text)\n                if ans is not None:\n                    answers_collected.append(ans)\n                    counts = Counter(answers_collected)\n                    if counts:\n                        most_common_ans, count = counts.most_common(1)[0]\n    \n                        if count > majority_threshold:\n                            print(f\"üéØ Majority reached! {most_common_ans} appeared {count} times\")\n                            stop_event.set()\n    \n                            # best-effort: cancel those not started yet\n                            for f in futures:\n                                if f is not fut and not f.done():\n                                    try:\n                                        f.cancel()\n                                    except Exception:\n                                        pass\n                            break\n    \n        except Exception as e:\n            import traceback\n            print(f\"Error in _inference_parallel: {e}\")\n            print(f\"Traceback: {traceback.format_exc()}\")\n        finally:\n            stop_event.set()\n            # Ensure all futures are handled\n            for fut in futures:\n                if not fut.done():\n                    try:\n                        fut.cancel()\n                    except Exception:\n                        pass\n            \n            # Shutdown executor with timeout protection\n            try:\n                # Python 3.9+ supports timeout, but we'll use a workaround for compatibility\n                import sys\n                if sys.version_info >= (3, 9):\n                    executor.shutdown(wait=True, timeout=60.0, cancel_futures=True)\n                else:\n                    # For older Python versions, use wait without timeout\n                    executor.shutdown(wait=True)\n            except TypeError:\n                # timeout parameter not supported, use without it\n                try:\n                    executor.shutdown(wait=True)\n                except Exception:\n                    executor.shutdown(wait=False)\n            except Exception as e:\n                print(f\"Warning: executor shutdown had issues: {e}\")\n                # Force shutdown\n                try:\n                    executor.shutdown(wait=False)\n                except Exception:\n                    pass\n    \n        return raw_responses\n\n\n    def extract_boxed_text(self, text: str) -> int | None:\n        \"\"\"Extract a numeric answer from '\\\\boxed{}' or 'final answer is ...' in the text.\"\"\"\n        # Pattern for \\boxed{NUMBER}\n        pattern = r'oxed{(.*?)}'\n        matches = re.findall(pattern, str(text))\n        if matches:\n            for match in reversed(matches):\n                if match:\n                    try:\n                        # Remove commas/spaces and parse as number (float covers scientific notation if any)\n                        clean_match = match.strip().replace(',', '').replace(' ', '')\n                        val = int(float(clean_match[:20]))\n                        if 0 <= val <= 99999:\n                            return val\n                    except Exception:\n                        pass\n\n        # Pattern for \"final answer is X\" or \"Final Answer: X\"\n        pattern = r'(?i)final\\s+answer\\s*(?:is|:)?\\s*(\\d+)'\n        matches = re.findall(pattern, text)\n        if matches:\n            for match in reversed(matches):\n                if match:\n                    try:\n                        val = int(match)\n                        if 0 <= val <= 99999:\n                            return val\n                    except Exception:\n                        pass\n\n        return None\n\n    def parse_responses(self, responses: list[str]) -> int:\n        \"\"\"Decide on the final answer from all responses by majority vote (with tie-break).\"\"\"\n        answers = [self.extract_boxed_text(r) for r in responses]\n\n        # Filter out any None values (cases where no answer was extracted)\n        valid_answers = [a for a in answers if a is not None]\n        if not valid_answers:\n            print(\"No valid answers found\")\n            return 8687\n\n        counter = Counter(valid_answers)\n        print(f\"Answers: {counter}\")\n\n        # Majority vote: pick the most common answer; break ties by choosing the largest answer\n        most_common_list = counter.most_common(2)\n        if len(most_common_list) > 1 and most_common_list[0][1] == most_common_list[1][1]:\n            tied_answers = [ans for ans, cnt in counter.items() if cnt == most_common_list[0][1]]\n            answer = max(tied_answers)\n        else:\n            answer = most_common_list[0][0]\n        return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.846560Z","iopub.execute_input":"2026-01-05T15:32:38.846680Z","iopub.status.idle":"2026-01-05T15:32:38.875649Z","shell.execute_reply.started":"2026-01-05T15:32:38.846670Z","shell.execute_reply":"2026-01-05T15:32:38.875227Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# time_budget_manager = DynamicTimeBudget(TOTAL_TIME, NUM_QUESTIONS, BUFFER_TIME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.876091Z","iopub.execute_input":"2026-01-05T15:32:38.876217Z","iopub.status.idle":"2026-01-05T15:32:38.890496Z","shell.execute_reply.started":"2026-01-05T15:32:38.876208Z","shell.execute_reply":"2026-01-05T15:32:38.890106Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialize the inferencer with the model path and parameters\ninferencer = HarmonyTIRInferencer(\n    \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n    use_budget=USE_BUDGET,\n    k=K,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:38.890940Z","iopub.execute_input":"2026-01-05T15:32:38.891067Z","iopub.status.idle":"2026-01-05T15:32:40.246495Z","shell.execute_reply.started":"2026-01-05T15:32:38.891058Z","shell.execute_reply":"2026-01-05T15:32:40.246067Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"inferencer.wait_server()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:32:40.246984Z","iopub.execute_input":"2026-01-05T15:32:40.247106Z","iopub.status.idle":"2026-01-05T15:36:42.512360Z","shell.execute_reply.started":"2026-01-05T15:32:40.247097Z","shell.execute_reply":"2026-01-05T15:36:42.511895Z"}},"outputs":[{"name":"stdout","text":"SyncPage[Model](data=[Model(id='gpt-oss', created=1767627402, object='model', owned_by='vllm', root='/kaggle/input/gpt-oss-120b/transformers/default/1', parent=None, max_model_len=65536, permission=[{'id': 'modelperm-3b465357514a4233bca2981c7e3d2ae8', 'object': 'model_permission', 'created': 1767627402, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import os\nimport subprocess\n\n# Ê£ÄÊü•CUDAÁõ∏ÂÖ≥ÁéØÂ¢ÉÂèòÈáè\nprint(\"üìã Checking CUDA environment:\")\nprint(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\nprint(f\"LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', 'Not set')}\")\n\n# Ê£ÄÊü•CUDAÁâàÊú¨\ntry:\n    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n    print(f\"CUDA version check: {result.returncode}\")\n    if result.returncode == 0:\n        print(result.stdout[:200])\nexcept:\n    print(\"nvcc not found\")\n\n# Ê£ÄÊü•nvidia-smi\ntry:\n    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n    print(f\"nvidia-smi output (first 10 lines):\")\n    for i, line in enumerate(result.stdout.split('\\n')[:10]):\n        print(f\"  {line}\")\nexcept Exception as e:\n    print(f\"nvidia-smi failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:36:42.512882Z","iopub.execute_input":"2026-01-05T15:36:42.513205Z","iopub.status.idle":"2026-01-05T15:36:42.615871Z","shell.execute_reply.started":"2026-01-05T15:36:42.513193Z","shell.execute_reply":"2026-01-05T15:36:42.615446Z"}},"outputs":[{"name":"stdout","text":"üìã Checking CUDA environment:\nCUDA_VISIBLE_DEVICES: 0\nLD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\nCUDA version check: 0\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Thu_Jun__6_02:18:23_PDT_2024\nCuda compilation tools, release 12.5, V12.5.82\nBuild cuda_12.5.r12.5/compiler.343\nnvidia-smi output (first 10 lines):\n  Mon Jan  5 15:36:42 2026       \n  +-----------------------------------------------------------------------------------------+\n  | NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n  |-----------------------------------------+------------------------+----------------------+\n  | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n  | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n  |                                         |                        |               MIG M. |\n  |=========================================+========================+======================|\n  |   0  NVIDIA H100 80GB HBM3          Off |   00000000:04:00.0 Off |                    0 |\n  | N/A   39C    P0            126W /  700W |   79663MiB /  81559MiB |      0%      Default |\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"init_time = time.time()\ncutoff_times = [int(x) for x in np.linspace(final_cutoff_time, init_time, 50 + 1)]\ncutoff_times.pop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:36:42.616335Z","iopub.execute_input":"2026-01-05T15:36:42.616451Z","iopub.status.idle":"2026-01-05T15:36:42.620034Z","shell.execute_reply.started":"2026-01-05T15:36:42.616442Z","shell.execute_reply":"2026-01-05T15:36:42.619650Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"1767627402"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    global correct_count, total_count, predictions, cutoff_times\n    \n    question_id = id_.item(0)\n    question_text = question.item(0)\n\n    print(\"------\")\n    print(f\"ID: {question_id}\")\n    print(f\"Question: {question_text[:200]}...\")\n\n    current_deadline = cutoff_times[-1]\n    answer,saved_time = inferencer.inference(question_text, deadline=current_deadline)\n    cutoff_times.pop()\n\n    # ‚è±Ô∏è Dynamically recompute cutoff_times and distribute saved_time\n    if len(cutoff_times) > 0:\n        now = time.time()\n        num_remaining = len(cutoff_times)\n        base_times = np.linspace(final_cutoff_time, now, num_remaining + 1)\n        base_times = base_times[:-1]  # keep only N timestamps\n        extra = saved_time / num_remaining\n        cutoff_times = [int(t + extra) for t in base_times]\n\n    # Store prediction\n    predictions[question_id] = answer\n    \n    # Check accuracy if ground truth available\n    total_count += 1\n    if question_id in ground_truth:\n        gt = ground_truth[question_id]\n        is_correct = (answer == gt)\n        if is_correct:\n            correct_count += 1\n        status = \"‚úÖ\" if is_correct else \"‚ùå\"\n        print(f\"Answer: {answer} | Ground Truth: {gt} | {status}\")\n        print(f\"üìä Running Accuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\")\n    else:\n        print(f\"Answer: {answer}\")\n    \n    print(\"------\\n\")\n\n    return pl.DataFrame({\"id\": question_id, \"answer\": answer})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:36:42.620474Z","iopub.execute_input":"2026-01-05T15:36:42.620595Z","iopub.status.idle":"2026-01-05T15:36:42.632521Z","shell.execute_reply.started":"2026-01-05T15:36:42.620585Z","shell.execute_reply":"2026-01-05T15:36:42.632102Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Load reference data and keep ground truth for accuracy calculation\ndf = pd.read_csv(\n    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n)\n\n# Store ground truth answers for accuracy calculation (only in local mode)\nground_truth = dict(zip(df[\"id\"], df[\"answer\"])) if \"answer\" in df.columns else {}\n\n# Create input file without answers\ndf.drop(\"answer\", axis=1, errors=\"ignore\").to_csv(\"reference.csv\", index=False)\n\n# Track predictions for accuracy calculation\npredictions = {}\ncorrect_count = 0\ntotal_count = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:36:42.632994Z","iopub.execute_input":"2026-01-05T15:36:42.633114Z","iopub.status.idle":"2026-01-05T15:36:42.674395Z","shell.execute_reply.started":"2026-01-05T15:36:42.633105Z","shell.execute_reply":"2026-01-05T15:36:42.673967Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import kaggle_evaluation.aimo_3_inference_server\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway((\"reference.csv\",))\n    \n    # Print final accuracy summary\n    if ground_truth and total_count > 0:\n        print(\"\\n\" + \"=\" * 50)\n        print(\"üìä FINAL ACCURACY SUMMARY\")\n        print(\"=\" * 50)\n        print(f\"Correct: {correct_count}/{total_count}\")\n        print(f\"Accuracy: {100*correct_count/total_count:.1f}%\")\n        print(\"=\" * 50)\n        \n        # Show details\n        print(\"\\nDetails:\")\n        for qid, pred in predictions.items():\n            if qid in ground_truth:\n                gt = ground_truth[qid]\n                status = \"‚úÖ\" if pred == gt else \"‚ùå\"\n                print(f\"  {qid}: pred={pred}, gt={gt} {status}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:36:42.674883Z","iopub.execute_input":"2026-01-05T15:36:42.675015Z","iopub.status.idle":"2026-01-05T16:14:52.210065Z","shell.execute_reply.started":"2026-01-05T15:36:42.675006Z","shell.execute_reply":"2026-01-05T16:14:52.209600Z"}},"outputs":[{"name":"stdout","text":"------\nID: 0e644e\nQuestion: Let $ABC$ be an acute-angled triangle with integer side lengths and $AB<AC$. Points $D$ and $E$ lie on segments $BC$ and $AC$, respectively, such that $AD=AE=AB$. Line $DE$ intersects $AB$ at $X$. Cir...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\n","output_type":"stream"},{"name":"stderr","text":"0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n[ColabKernelApp] WARNING | Eventloop or matplotlib integration failed. Is matplotlib installed?\n","output_type":"stream"},{"name":"stdout","text":"üéØ Majority reached! 336 appeared 5 times\n[Budget]: 347.44s\n[inference] Took 186.70s\n[Saved time]: 160.74s\nAnswers: Counter({336: 5})\nAnswer: 336 | Ground Truth: 336 | ‚úÖ\nüìä Running Accuracy: 1/1 (100.0%)\n------\n\n------\nID: 424e18\nQuestion: A tournament is held with $2^{20}$ runners each of which has a different running speed. In each race, two runners compete against each other with the faster runner always winning the race. The competi...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 21818 appeared 5 times\n[Budget]: 354.66s\n[inference] Took 232.21s\n[Saved time]: 122.45s\nAnswers: Counter({21818: 5, 62140: 1})\nAnswer: 21818 | Ground Truth: 21818 | ‚úÖ\nüìä Running Accuracy: 2/2 (100.0%)\n------\n\n------\nID: 26de63\nQuestion: Define a function $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$ by\n\\begin{equation*}\n    f(n) = \\sum_{i = 1}^n \\sum_{j = 1}^n j^{1024} \\left\\lfloor\\frac1j + \\frac{n-i}{n}\\right\\rfloor.\n\\end{e...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 32951 appeared 5 times\n[Budget]: 356.45s\n[inference] Took 73.71s\n[Saved time]: 282.74s\nAnswers: Counter({32951: 5})\nAnswer: 32951 | Ground Truth: 32951 | ‚úÖ\nüìä Running Accuracy: 3/3 (100.0%)\n------\n\n------\nID: 86e8e5\nQuestion: Let $n \\geq 6$ be a positive integer. We call a positive integer $n$-Norwegian if it has three distinct positive divisors whose sum is equal to $n$. Let $f(n)$ denote the smallest $n$-Norwegian positi...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\n‚ö†Ô∏è Error creating/reading stream: timed out\n[Budget]: 366.74s\n[inference] Took 366.99s\n[Saved time]: 0.00s\nNo valid answers found\nAnswer: 8687 | Ground Truth: 8687 | ‚úÖ\nüìä Running Accuracy: 4/4 (100.0%)\n------\n\n------\nID: 9c1c5f\nQuestion: Let $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$ be a function such that for all positive integers $m$ and $n$, \n\\begin{equation*}\n    f(m) + f(n) = f(m + n + mn).\n\\end{equation*}\nAcross all...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 580 appeared 5 times\nüõë Stop signal received\n[Budget]: 359.74s\n[inference] Took 189.10s\n[Saved time]: 170.64s\nAnswers: Counter({580: 5, 454: 1})\nAnswer: 580 | Ground Truth: 580 | ‚úÖ\nüìä Running Accuracy: 5/5 (100.0%)\n------\n\n------\nID: dd7f5e\nQuestion: Let $\\mathcal{F}$ be the set of functions $\\alpha \\colon \\mathbb{Z}\\to \\mathbb{Z}$ for which there are only finitely many $n \\in \\mathbb{Z}$ such that $\\alpha(n) \\neq 0$. \n\nFor two functions $\\alpha$ ...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 160 appeared 5 times\n[Budget]: 367.63s\n[inference] Took 305.86s\n[Saved time]: 61.77s\nAnswers: Counter({160: 5})\nAnswer: 160 | Ground Truth: 160 | ‚úÖ\nüìä Running Accuracy: 6/6 (100.0%)\n------\n\n------\nID: 92ba6a\nQuestion: Alice and Bob are each holding some integer number of sweets. Alice says to Bob: ``If we each added the number of sweets we're holding to our (positive integer) age, my answer would be double yours. I...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 50 appeared 5 times\n[Budget]: 366.77s\n[inference] Took 43.88s\n[Saved time]: 322.89s\nAnswers: Counter({50: 5})\nAnswer: 50 | Ground Truth: 50 | ‚úÖ\nüìä Running Accuracy: 7/7 (100.0%)\n------\n\n------\nID: a295e9\nQuestion: A $500 \\times 500$ square is divided into $k$ rectangles, each having integer side lengths. Given that no two of these rectangles have the same perimeter, the largest possible value of $k$ is $\\mathca...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\n‚è∞ Not enough remaining time for python (0.20s)\n‚è∞ Not enough remaining time for python (0.00s)\n[Budget]: 379.88s\n[inference] Took 380.09s\n[Saved time]: 0.00s\nAnswers: Counter({520: 1})\nAnswer: 520 | Ground Truth: 520 | ‚úÖ\nüìä Running Accuracy: 8/8 (100.0%)\n------\n\n------\nID: 42d360\nQuestion: On a blackboard, Ken starts off by writing a positive integer $n$ and then applies the following move until he first reaches $1$. Given that the number on the board is $m$, he chooses a base $b$, wher...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\nüéØ Majority reached! 32193 appeared 5 times\n[Budget]: 372.78s\n[inference] Took 125.17s\n[Saved time]: 247.62s\nAnswers: Counter({32193: 5})\nAnswer: 32193 | Ground Truth: 32193 | ‚úÖ\nüìä Running Accuracy: 9/9 (100.0%)\n------\n\n------\nID: 641659\nQuestion: Let $ABC$ be a triangle with $AB \\neq AC$, circumcircle $\\Omega$, and incircle $\\omega$. Let the contact points of $\\omega$ with $BC$, $CA$, and $AB$ be $D$, $E$, and $F$, respectively. Let the circum...\nBudget disabled -> N: 8\nüöÄ Sampling 8 times (threshold: > 4.0)...\n‚è∞ Not enough remaining time for python (0.00s)\n[Budget]: 384.61s\n[inference] Took 384.80s\n[Saved time]: 0.00s\nAnswers: Counter({57447: 3})\nAnswer: 57447 | Ground Truth: 57447 | ‚úÖ\nüìä Running Accuracy: 10/10 (100.0%)\n------\n\n\n==================================================\nüìä FINAL ACCURACY SUMMARY\n==================================================\nCorrect: 10/10\nAccuracy: 100.0%\n==================================================\n\nDetails:\n  0e644e: pred=336, gt=336 ‚úÖ\n  424e18: pred=21818, gt=21818 ‚úÖ\n  26de63: pred=32951, gt=32951 ‚úÖ\n  86e8e5: pred=8687, gt=8687 ‚úÖ\n  9c1c5f: pred=580, gt=580 ‚úÖ\n  dd7f5e: pred=160, gt=160 ‚úÖ\n  92ba6a: pred=50, gt=50 ‚úÖ\n  a295e9: pred=520, gt=520 ‚úÖ\n  42d360: pred=32193, gt=32193 ‚úÖ\n  641659: pred=57447, gt=57447 ‚úÖ\n","output_type":"stream"}],"execution_count":24}]}